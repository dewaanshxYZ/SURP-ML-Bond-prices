{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "Unlike the Equity Market, the Bond Market suffers from a severe lack of information.  With no quotes publicly available, the best way to get a quote is to solicit multiple brokers and wait for a reply.  Benchmark Solutions plans to predict accurate prices that incorporate interest rate data, trades or quotes of the bond in question, trades or quotes of other bonds or CDS of the issuer of the bond in question as well as other input sources. \n",
    "\n",
    "#### Background\n",
    "While the initial competition was 6 years ago, several individuals have continued to develop models to address this problem. \n",
    "\n",
    ">The winning kaggle team ‘upbeat’ was able to predict prices with .6803 MAE in 2012.\n",
    "    \n",
    ">In 2017, Swetava Ganguli and Jared Dunnmon published a paper using a variety of machine learning models.  Their best score was .6668 WEPS.\n",
    "\n",
    "Reading the specifics of the Ganguli, Dunnmon paper and reviewing their results I decided to use multilayer Neural Networks and GLMs to create better and faster predictions.\n",
    "\n",
    "This table is the results of the Ganguli, Dunnmon paper.  In their review of the results the authors concluded that “NNs and GLMs give best results in terms of combined speed and accuracy.” And “the success of neural networks on this dataset implies that investigating the application of multilayer networks and deep learning methods to this problem may yield better bond price predictions.”\n",
    "\n",
    "<img src=\"./results.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/train.csv')\n",
    "submit = pd.read_csv('./Data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data & Cleaning\n",
    "\n",
    "Instantiating our data from the Benchmark Solution Kaggle Data (https://www.kaggle.com/c/benchmark-bond-trade-price-challenge/data).  While the data set is generally clean, there were a couple steps to go further with cleaning.  Notably, dummying out the categorical varaiables and filling the time-series NaN lags with -1000 so they can be utilized in the model. \n",
    "\n",
    "This is some information about the data and columns:\n",
    "US corporate bond trade data is provided. Each row includes trade details, some basic information about the traded bond, and information about the previous 10 trades. Contestants are asked to predict trade price.\n",
    "\n",
    "Column details:\n",
    "1. id: The row id. bond_id: The unique id of a bond to aid in timeseries reconstruction. (This column is only present in the train data)\n",
    "2. trade_price: The price at which the trade occured. (This is the column to predict in the test data)\n",
    "3. weight: The weight of the row for evaluation purposes. This is calculated as the square root of the time since the last trade and then scaled so the mean is 1.\n",
    "4. current_coupon: The coupon of the bond at the time of the trade.\n",
    "5. time_to_maturity: The number of years until the bond matures at the time of the trade.\n",
    "6. is_callable: A binary value indicating whether or not the bond is callable by the issuer.\n",
    "7. reporting_delay: The number of seconds after the trade occured that it was reported.\n",
    "8. trade_size: The notional amount of the trade.\n",
    "9. trade_type: 2=customer sell, 3=customer buy, 4=trade between dealers. We would expect customers to get worse prices on average than dealers.\n",
    "10. curve_based_price: A fair price estimate based on implied hazard and funding curves of the issuer of the bond.\n",
    "\n",
    "11. received_time_diff_last{1-10}: The time difference between the trade and that of the previous {1-10}.\n",
    "21. trade_price_last{1-10}: The trade price of the last {1-10} trades.\n",
    "31. trade_size_last{1-10}: The notional amount of the last {1-10} trades.\n",
    "41. trade_type_last{1-10}: The trade type of the last {1-10} trades.\n",
    "51. curve_based_price_last{1-10}: The curve based price of the last {1-10} trades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(df['trade_type'])\n",
    "df = pd.concat([df,dummies], axis = 1)\n",
    "df = df.drop('trade_type', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('id',axis = 1,inplace = True)\n",
    "\n",
    "df.fillna(-1000,inplace = True)\n",
    "submit.fillna(-1000,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing and feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbaca\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA for Feature Selection\n",
    "After the data was prepared, I used principal component analysis (PCA) on the entire data set in order to see if there existed a reduced feature set that retains the majority of the explanatory power of the full feature set.  Other typical preprocessing steps were taken, including train-test-split and standard scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('trade_price', axis = 1)\n",
    "y = df['trade_price']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submit = submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_X = pca.fit_transform(X)\n",
    "#pca_submit = pca.transform(X_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X = ss.fit_transform(pca_X)\n",
    "#final_submit = ss.transform(pca_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbaca\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_X,y,test_size = .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "My initial model was based on the suggestions of Ganguli and Dunnmon using a 5 layer Neural network and editing the number of nodes per layer.\n",
    "Second and third models were GLMs which included an ordinary least squares regression (OLS) and a weighted least squares regression (WLS)\n",
    "My fourth model was a Long Short-Term Memory Neural Network of similar design to my original model - 5 layers with varying node sizes.\n",
    "I included a random forest regressor as a fifth additional option.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Networks\n",
    "\n",
    "This first model is my RNN, with 5 layers of varying size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 2.5525 - val_loss: 0.7491\n",
      "Epoch 2/100\n",
      "381339/381339 [==============================] - 23s 60us/step - loss: 0.8019 - val_loss: 0.8324\n",
      "Epoch 3/100\n",
      "381339/381339 [==============================] - 23s 60us/step - loss: 0.7422 - val_loss: 0.7000\n",
      "Epoch 4/100\n",
      "381339/381339 [==============================] - 23s 60us/step - loss: 0.7128 - val_loss: 0.7331\n",
      "Epoch 5/100\n",
      "381339/381339 [==============================] - 23s 59us/step - loss: 0.6936 - val_loss: 0.7203\n",
      "Epoch 6/100\n",
      "381339/381339 [==============================] - 23s 60us/step - loss: 0.6690 - val_loss: 0.5951\n",
      "Epoch 7/100\n",
      "381339/381339 [==============================] - 23s 60us/step - loss: 0.6549 - val_loss: 0.6390\n",
      "Epoch 8/100\n",
      "381339/381339 [==============================] - 23s 60us/step - loss: 0.6365 - val_loss: 0.5736\n",
      "Epoch 9/100\n",
      "381339/381339 [==============================] - 23s 59us/step - loss: 0.6243 - val_loss: 0.5927\n",
      "Epoch 10/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.6124 - val_loss: 0.5811\n",
      "Epoch 11/100\n",
      "381339/381339 [==============================] - 22s 59us/step - loss: 0.6019 - val_loss: 0.5526\n",
      "Epoch 12/100\n",
      "381339/381339 [==============================] - 22s 59us/step - loss: 0.5901 - val_loss: 0.5382\n",
      "Epoch 13/100\n",
      "381339/381339 [==============================] - 22s 57us/step - loss: 0.5793 - val_loss: 0.5596\n",
      "Epoch 14/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.5701 - val_loss: 0.5465\n",
      "Epoch 15/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.5641 - val_loss: 0.6898\n",
      "Epoch 16/100\n",
      "381339/381339 [==============================] - 21s 56us/step - loss: 0.5560 - val_loss: 0.5146\n",
      "Epoch 17/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.5471 - val_loss: 0.5045\n",
      "Epoch 18/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.5407 - val_loss: 0.5324\n",
      "Epoch 19/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.5323 - val_loss: 0.5697\n",
      "Epoch 20/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.5294 - val_loss: 0.4993\n",
      "Epoch 21/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.5200 - val_loss: 0.5125\n",
      "Epoch 22/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.5169 - val_loss: 0.4883\n",
      "Epoch 23/100\n",
      "381339/381339 [==============================] - 22s 57us/step - loss: 0.5098 - val_loss: 0.4742\n",
      "Epoch 24/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.5068 - val_loss: 0.4765\n",
      "Epoch 25/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.5037 - val_loss: 0.4996\n",
      "Epoch 26/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.5001 - val_loss: 0.4796\n",
      "Epoch 27/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.4965 - val_loss: 0.5214\n",
      "Epoch 28/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.4925 - val_loss: 0.4817\n",
      "Epoch 29/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.4916 - val_loss: 0.4637\n",
      "Epoch 30/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.4885 - val_loss: 0.5442\n",
      "Epoch 31/100\n",
      "381339/381339 [==============================] - 22s 57us/step - loss: 0.4871 - val_loss: 0.4765\n",
      "Epoch 32/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.4851 - val_loss: 0.4619\n",
      "Epoch 33/100\n",
      "381339/381339 [==============================] - 21s 55us/step - loss: 0.4819 - val_loss: 0.4560\n",
      "Epoch 34/100\n",
      "381339/381339 [==============================] - 23s 60us/step - loss: 0.4807 - val_loss: 0.4944\n",
      "Epoch 35/100\n",
      "381339/381339 [==============================] - 21s 56us/step - loss: 0.4778 - val_loss: 0.4491\n",
      "Epoch 36/100\n",
      "381339/381339 [==============================] - 21s 56us/step - loss: 0.4773 - val_loss: 0.5231\n",
      "Epoch 37/100\n",
      "381339/381339 [==============================] - 22s 59us/step - loss: 0.4744 - val_loss: 0.4541\n",
      "Epoch 38/100\n",
      "381339/381339 [==============================] - 24s 63us/step - loss: 0.4739 - val_loss: 0.4876\n",
      "Epoch 39/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.4708 - val_loss: 0.5075\n",
      "Epoch 40/100\n",
      "381339/381339 [==============================] - 26s 67us/step - loss: 0.4710 - val_loss: 0.4570\n",
      "Epoch 41/100\n",
      "381339/381339 [==============================] - 23s 60us/step - loss: 0.4689 - val_loss: 0.4525\n",
      "Epoch 42/100\n",
      "381339/381339 [==============================] - 26s 68us/step - loss: 0.4657 - val_loss: 0.4740\n",
      "Epoch 43/100\n",
      "381339/381339 [==============================] - 24s 63us/step - loss: 0.4671 - val_loss: 0.4405\n",
      "Epoch 44/100\n",
      "381339/381339 [==============================] - 24s 64us/step - loss: 0.4653 - val_loss: 0.4770\n",
      "Epoch 45/100\n",
      "381339/381339 [==============================] - 22s 59us/step - loss: 0.4671 - val_loss: 0.4866\n",
      "Epoch 46/100\n",
      "381339/381339 [==============================] - 24s 63us/step - loss: 0.4669 - val_loss: 0.4506\n",
      "Epoch 47/100\n",
      "381339/381339 [==============================] - 23s 61us/step - loss: 0.4632 - val_loss: 0.4575\n",
      "Epoch 48/100\n",
      "381339/381339 [==============================] - 23s 60us/step - loss: 0.4626 - val_loss: 0.4388\n",
      "Epoch 49/100\n",
      "381339/381339 [==============================] - 23s 59us/step - loss: 0.4637 - val_loss: 0.5084\n",
      "Epoch 50/100\n",
      "381339/381339 [==============================] - 23s 59us/step - loss: 0.4621 - val_loss: 0.4454\n",
      "Epoch 51/100\n",
      "381339/381339 [==============================] - 23s 60us/step - loss: 0.4632 - val_loss: 0.4604\n",
      "Epoch 52/100\n",
      "381339/381339 [==============================] - 23s 59us/step - loss: 0.4614 - val_loss: 0.4442\n",
      "Epoch 53/100\n",
      "381339/381339 [==============================] - 24s 62us/step - loss: 0.4609 - val_loss: 0.4539\n",
      "Epoch 54/100\n",
      "381339/381339 [==============================] - 23s 60us/step - loss: 0.4615 - val_loss: 0.4371\n",
      "Epoch 55/100\n",
      "381339/381339 [==============================] - 22s 57us/step - loss: 0.4613 - val_loss: 0.5395\n",
      "Epoch 56/100\n",
      "381339/381339 [==============================] - 19s 51us/step - loss: 0.4602 - val_loss: 0.4987\n",
      "Epoch 57/100\n",
      "381339/381339 [==============================] - 21s 55us/step - loss: 0.4587 - val_loss: 0.5087\n",
      "Epoch 58/100\n",
      "381339/381339 [==============================] - 23s 60us/step - loss: 0.4587 - val_loss: 0.4554\n",
      "Epoch 59/100\n",
      "381339/381339 [==============================] - 23s 59us/step - loss: 0.4575 - val_loss: 0.4883\n",
      "Epoch 60/100\n",
      "381339/381339 [==============================] - 23s 60us/step - loss: 0.4594 - val_loss: 0.4378\n",
      "Epoch 61/100\n",
      "381339/381339 [==============================] - 23s 60us/step - loss: 0.4578 - val_loss: 0.4545\n",
      "Epoch 62/100\n",
      "381339/381339 [==============================] - 23s 61us/step - loss: 0.4571 - val_loss: 0.5189\n",
      "Epoch 63/100\n",
      "381339/381339 [==============================] - 23s 61us/step - loss: 0.4571 - val_loss: 0.4523\n",
      "Epoch 64/100\n",
      "381339/381339 [==============================] - 24s 64us/step - loss: 0.4574 - val_loss: 0.4353\n",
      "Epoch 65/100\n",
      "381339/381339 [==============================] - 23s 60us/step - loss: 0.4575 - val_loss: 0.4480\n",
      "Epoch 66/100\n",
      "381339/381339 [==============================] - 22s 59us/step - loss: 0.4561 - val_loss: 0.4425\n",
      "Epoch 67/100\n",
      "381339/381339 [==============================] - 22s 57us/step - loss: 0.4556 - val_loss: 0.4517\n",
      "Epoch 68/100\n",
      "381339/381339 [==============================] - 24s 62us/step - loss: 0.4549 - val_loss: 0.4983\n",
      "Epoch 69/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.4554 - val_loss: 0.4741\n",
      "Epoch 70/100\n",
      "381339/381339 [==============================] - 23s 61us/step - loss: 0.4548 - val_loss: 0.4908\n",
      "Epoch 71/100\n",
      "381339/381339 [==============================] - 23s 61us/step - loss: 0.4561 - val_loss: 0.4435\n",
      "Epoch 72/100\n",
      "381339/381339 [==============================] - 24s 63us/step - loss: 0.4538 - val_loss: 0.4545\n",
      "Epoch 73/100\n",
      "381339/381339 [==============================] - 25s 65us/step - loss: 0.4543 - val_loss: 0.4577\n",
      "Epoch 74/100\n",
      "381339/381339 [==============================] - 23s 61us/step - loss: 0.4519 - val_loss: 0.4542\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381339/381339 [==============================] - 25s 64us/step - loss: 0.4518 - val_loss: 0.4412\n",
      "Epoch 76/100\n",
      "381339/381339 [==============================] - 23s 59us/step - loss: 0.4522 - val_loss: 0.4317\n",
      "Epoch 77/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.4518 - val_loss: 0.4616\n",
      "Epoch 78/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.4511 - val_loss: 0.4623\n",
      "Epoch 79/100\n",
      "381339/381339 [==============================] - 23s 59us/step - loss: 0.4520 - val_loss: 0.4559\n",
      "Epoch 80/100\n",
      "381339/381339 [==============================] - 21s 54us/step - loss: 0.4516 - val_loss: 0.4552\n",
      "Epoch 81/100\n",
      "381339/381339 [==============================] - 21s 54us/step - loss: 0.4519 - val_loss: 0.4528\n",
      "Epoch 82/100\n",
      "381339/381339 [==============================] - 21s 55us/step - loss: 0.4510 - val_loss: 0.4468\n",
      "Epoch 83/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.4507 - val_loss: 0.4337\n",
      "Epoch 84/100\n",
      "381339/381339 [==============================] - 21s 56us/step - loss: 0.4511 - val_loss: 0.4514\n",
      "Epoch 85/100\n",
      "381339/381339 [==============================] - 22s 57us/step - loss: 0.4528 - val_loss: 0.4389\n",
      "Epoch 86/100\n",
      "381339/381339 [==============================] - 21s 56us/step - loss: 0.4510 - val_loss: 0.4342\n",
      "Epoch 87/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.4502 - val_loss: 0.4592\n",
      "Epoch 88/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.4494 - val_loss: 0.4350\n",
      "Epoch 89/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.4500 - val_loss: 0.4617\n",
      "Epoch 90/100\n",
      "381339/381339 [==============================] - 23s 59us/step - loss: 0.4502 - val_loss: 0.4338\n",
      "Epoch 91/100\n",
      "381339/381339 [==============================] - 25s 67us/step - loss: 0.4491 - val_loss: 0.4854\n",
      "Epoch 92/100\n",
      "381339/381339 [==============================] - 26s 69us/step - loss: 0.4495 - val_loss: 0.4417\n",
      "Epoch 93/100\n",
      "381339/381339 [==============================] - 22s 58us/step - loss: 0.4491 - val_loss: 0.4540\n",
      "Epoch 94/100\n",
      "381339/381339 [==============================] - 24s 64us/step - loss: 0.4480 - val_loss: 0.4373\n",
      "Epoch 95/100\n",
      "381339/381339 [==============================] - 23s 61us/step - loss: 0.4490 - val_loss: 0.4746\n",
      "Epoch 96/100\n",
      "381339/381339 [==============================] - 22s 59us/step - loss: 0.4490 - val_loss: 0.4360\n",
      "Epoch 97/100\n",
      "381339/381339 [==============================] - 23s 61us/step - loss: 0.4485 - val_loss: 0.4306\n",
      "Epoch 98/100\n",
      "381339/381339 [==============================] - 21s 56us/step - loss: 0.4475 - val_loss: 0.4387\n",
      "Epoch 99/100\n",
      "381339/381339 [==============================] - 22s 59us/step - loss: 0.4479 - val_loss: 0.5368\n",
      "Epoch 100/100\n",
      "381339/381339 [==============================] - 23s 61us/step - loss: 0.4487 - val_loss: 0.4384\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "input_units = X_train.shape[1]\n",
    "hidden_units = input_units\n",
    "\n",
    "model.add(Dense(hidden_units, input_dim=input_units, activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "#model.add(Dropout())\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "adam = Adam(lr=0.001)\n",
    "model.compile(loss='mean_absolute_error', optimizer=adam)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "          epochs=100, batch_size=None)\n",
    "\n",
    "#Add Dropout Layers (ask Matt)\n",
    "#Ask about Batch Size\n",
    "#RNN potentially before PCA and Scaling - based on correlations from above\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Epoch')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "plt.plot(train_loss, label='Training loss', c = 'g', lw = 3)\n",
    "plt.plot(test_loss, label='Testing loss',c = 'b', lw = 3)\n",
    "plt.legend()\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-094396618eb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mRNNpredsTest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mRNNpredsTrai\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "RNNpredsTest = model.predict(X_test)\n",
    "RNNpredsTrain = model.predict(X_train)\n",
    "\n",
    "RNNTestScore = mean_absolute_error(y_test, RNNpredsTest)\n",
    "RNNTrainScore = mean_absolute_error(y_train, RNNpredsTrain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM\n",
    "This is the next NN.  The idea with the Long Short-Term Memory model is that the time-series data would provide better predictability and better results.  Important to note, the factor function is used to determine batch size, LSTMs require batches to be divisible by the sample shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_reshape = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "test_X_reshape= np.reshape(X_test, (X_test.shape[0],X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "7\n",
      "9\n",
      "21\n",
      "63\n",
      "6053\n",
      "18159\n",
      "42371\n",
      "54477\n",
      "127113\n",
      "381339\n"
     ]
    }
   ],
   "source": [
    "def print_factors(x):\n",
    "    for i in range(1, x + 1):\n",
    "        if x % i == 0:\n",
    "            print(i)\n",
    "            \n",
    "print_factors(train_X_reshape.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1501.3346456692914"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_reshape.shape[0]/254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 178s 466us/step - loss: 5.5772 - val_loss: 3.4522\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 160s 420us/step - loss: 2.3503 - val_loss: 1.7098\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 158s 415us/step - loss: 2.0631 - val_loss: 3.8997\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 166s 436us/step - loss: 1.9425 - val_loss: 1.1918\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 164s 431us/step - loss: 1.9166 - val_loss: 3.0448\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 159s 416us/step - loss: 2.1843 - val_loss: 1.7221\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 160s 421us/step - loss: 1.5467 - val_loss: 1.3484\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 161s 423us/step - loss: 1.3551 - val_loss: 1.2135\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 158s 415us/step - loss: 1.2439 - val_loss: 1.3680\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 173s 453us/step - loss: 1.1630 - val_loss: 1.1183\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 173s 453us/step - loss: 1.1007 - val_loss: 1.0379\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 171s 448us/step - loss: 1.0650 - val_loss: 1.0749\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 170s 445us/step - loss: 1.0347 - val_loss: 0.9760\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 171s 449us/step - loss: 1.0055 - val_loss: 0.9320\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 169s 443us/step - loss: 0.9803 - val_loss: 0.9854\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 167s 439us/step - loss: 0.9618 - val_loss: 0.9514\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 169s 442us/step - loss: 0.9387 - val_loss: 1.0370\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 177s 465us/step - loss: 0.9237 - val_loss: 0.8264\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 181s 475us/step - loss: 0.9095 - val_loss: 0.8349\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 169s 442us/step - loss: 0.8920 - val_loss: 0.8311\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 165s 433us/step - loss: 0.8789 - val_loss: 0.7988\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 160s 421us/step - loss: 0.8650 - val_loss: 0.7733\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 163s 427us/step - loss: 0.8587 - val_loss: 0.7788\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 168s 440us/step - loss: 0.8441 - val_loss: 0.7769\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 163s 427us/step - loss: 0.8443 - val_loss: 0.8278\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 167s 438us/step - loss: 0.8323 - val_loss: 0.8247\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 170s 446us/step - loss: 0.8203 - val_loss: 0.7816\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 169s 442us/step - loss: 0.8119 - val_loss: 0.7656\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 169s 443us/step - loss: 0.8059 - val_loss: 0.7847\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 169s 444us/step - loss: 0.7961 - val_loss: 0.7688\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 174s 456us/step - loss: 0.7878 - val_loss: 0.7660\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 179s 470us/step - loss: 0.7850 - val_loss: 0.7563\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 173s 453us/step - loss: 0.7782 - val_loss: 0.7610\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 170s 447us/step - loss: 0.7762 - val_loss: 0.7848\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 174s 457us/step - loss: 0.7687 - val_loss: 0.7600\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 172s 452us/step - loss: 2.6463 - val_loss: 3.3669\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 171s 448us/step - loss: 2.7171 - val_loss: 0.9876\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 173s 453us/step - loss: 0.9863 - val_loss: 0.8591\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 167s 439us/step - loss: 3.0871 - val_loss: 2.3429\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 170s 445us/step - loss: 2.4663 - val_loss: 2.0669\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 175s 459us/step - loss: 1.9762 - val_loss: 1.8069\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 170s 445us/step - loss: 1.8035 - val_loss: 1.7386\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 202s 529us/step - loss: 1.6878 - val_loss: 1.6095\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 229s 602us/step - loss: 1.6328 - val_loss: 1.5239\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 195s 510us/step - loss: 1.5626 - val_loss: 1.4755\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 171s 450us/step - loss: 1.4801 - val_loss: 1.4134\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 198s 518us/step - loss: 1.4249 - val_loss: 1.4065\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 195s 512us/step - loss: 1.3722 - val_loss: 1.5915\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 193s 506us/step - loss: 1.3521 - val_loss: 1.2542\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 176s 462us/step - loss: 1.2972 - val_loss: 1.2580\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 197s 516us/step - loss: 1.2656 - val_loss: 1.2602\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 186s 487us/step - loss: 1.2372 - val_loss: 1.1599\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 182s 476us/step - loss: 1.2118 - val_loss: 1.1496\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 183s 479us/step - loss: 1.1858 - val_loss: 1.1302\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 175s 458us/step - loss: 1.1649 - val_loss: 1.1099\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 181s 475us/step - loss: 1.1493 - val_loss: 1.0641\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 185s 485us/step - loss: 1.1240 - val_loss: 1.0472\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 175s 458us/step - loss: 1.1082 - val_loss: 1.0656\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "381339/381339 [==============================] - 183s 481us/step - loss: 1.0908 - val_loss: 1.0993\n",
      "Train on 381339 samples, validate on 381339 samples\n",
      "Epoch 1/1\n",
      "203364/381339 [==============>...............] - ETA: 1:13 - loss: 1.0788- ETA: 1:14 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-3539003c12b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     model.fit(train_X_reshape, y_train, validation_data=(test_X_reshape, y_test), \n\u001b[1;32m---> 18\u001b[1;33m               epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size = 63\n",
    "model = Sequential()\n",
    "model.add(LSTM(60, \n",
    "               batch_input_shape=(batch_size, train_X_reshape.shape[1], train_X_reshape.shape[2]), \n",
    "               stateful=True))\n",
    "model.add(Dense (300, activation='relu'))\n",
    "model.add(Dense (150, activation='relu'))\n",
    "model.add(Dense (75, activation='relu'))\n",
    "model.add(Dense (25, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "for i in range(100):\n",
    "    model.fit(train_X_reshape, y_train, validation_data=(test_X_reshape, y_test), \n",
    "              epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "plt.plot(train_loss, label='Training loss', c = 'g', lw = 3)\n",
    "plt.plot(test_loss, label='Testing loss',c = 'b', lw = 3)\n",
    "plt.legend()\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMpredsTest = model.predict(X_test)\n",
    "LSTMpredsTrain = model.predict(X_train)\n",
    "\n",
    "LSTMTestScore = mean_absolute_error(y_test, LSTMpredsTest)\n",
    "LSTMTrainScore = mean_absolute_error(y_train, LSTMpredsTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Linear Models\n",
    "The next series of models were GLMs, which I included Ordinary Least Square Regression and Weighted Least Square regression.  I instantiated a different train-test-split to redo some of the changes I made above and allowed for easy action with my models.\n",
    "\n",
    "\n",
    "\n",
    "#### OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "olsr = sm.OLS(y_train1,X_train1,family = sm.families.Gamma())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_olsr = olsr.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLSpredstrain = result_olsr.predict(X_train1)\n",
    "OLSpredstest = result_olsr.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLStrainscore =  mean_absolute_error(y_train1, prestrain)\n",
    "OLStestscore = mean_absolute_error(y_test1, pres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WLS\n",
    "Benchmark was able to provide a calculated weight column which made implimentiation of a WLSr as easy as including a dropped column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wlsr = sm.WLS(y_train1,X_train1.drop('weight',axis = 1),weights=X_train1['weight'], family= sm.families.Gamma())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "WLS = wlsr.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "wweight = X_test1.drop('weight', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "wweighttrain =X_train1.drop('weight',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlstestpreds = WLS.predict(wweight)\n",
    "wlstrainpreds = WLS.predict(wweighttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wlstest = mean_absolute_error(y_test1, wlstestpreds)\n",
    "wlstrain = mean_absolute_error(y_train1, wlstrainpreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that OLS AND WLS are not PCA'd or Scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "I included a random forest as an additional 5th model to test the perforance relative to the Ganguli, Dunnmon paper.  In keeping constant with the results of the paper I used 50 'n_estimators' to start.  I only added this at the end of my project and found the results to be so overfit I didn't explore them further.  Read more in the 'Results' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor(n_estimators=50)\n",
    "\n",
    "rfr.fit(X_train,y_train)\n",
    "rfrpredsTEST = rfr.predict(X_test)\n",
    "rfrpredstrain = rfr.predict(X_train)\n",
    "\n",
    "rfrtestscore = mean_absolute_error(y_test,rfrpredsTEST)\n",
    "rfrtrainscore = mean_absolute_error(y_train,rfrpredstrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "With the consideration of speed and accuracy I found that the multi-layer Neural Network (NN) produced the best results.  Reaching convergence around 100 epochs in just under 36 minutes and returning the lowest MAE.\n",
    "\n",
    "LSTM NN which came close to the first NN in accuracy but took substantially longer to run, reaching convergence at just over 100 epochs taking 5 hours and 22 minutes.\n",
    "\n",
    "OLS and WLS performed very quickly but fell short in accuracy.\n",
    "\n",
    "Random Forest had decent results but at the cost of speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorernn = {'Multilayer NN (5 hidden w/o Dropout)': [MAERNNTrain,MAERNNTest]}\n",
    "Results = pd.DataFrame(scorenn).T\n",
    "Results = Results.rename(columns = ({0:'Train',1:'Test'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorelstm = {'LSTM NN (5 hidden w/o Dropout)': [LSTMTrainScore,LSTMTestScore]}\n",
    "scorelstm = pd.DataFrame(scorelstm).T\n",
    "scorelstm = scorelstm.rename(columns = ({0:'Train',1:'Test'}))\n",
    "Results.append(scorelstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreols = {'OLS': [float(OLStrainscore),float(OLStestscore)]}\n",
    "scoreols = pd.DataFrame(scoreols).T\n",
    "scoreols = scoreols.rename(columns = ({0:'Train',1:'Test'}))\n",
    "Results.append(scoreols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorewls = {'WLS': [wlstrain, wlstest]}\n",
    "scorewls = pd.DataFrame(scorewls).T\n",
    "scorewls = scorewls.rename(columns = ({0:'Train',1:'Test'}))\n",
    "Results.append(scorewls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfrscore = {'Random Forest': [rfrtrainscore, rfrtestscore]}\n",
    "rfrscore = pd.DataFrame(rfrscore).T\n",
    "rfrscore = rfrscore.rename(columns = ({0:'Train',1:'Test'}))\n",
    "Results.append(scorerfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-56955730bedf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mResults\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Results' is not defined"
     ]
    }
   ],
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am satisfied with these results as they outperformed the other metrics of my competitors.  They also showed significant improvements in speed.   Even on my Intel core i7–7700hq I could run the majority of these models in under 2 hours.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the Future\n",
    "To develop this project further there are two directions I could take it to improve results an functionality.  \n",
    "> ##### More Data:\n",
    "While this dataset was extensive, being able to utilize more data could monumentally improve my models performance\n",
    ">##### Computational Power:\n",
    "In the testing process I was limited by my computer’s processing capability. My current processor Intel core i7–7700hq was quickly maxed out running only 1 NN.  I was able to run on both my GPU and CPU which improved my testing times but was still a substantial barrier in adjusting my tests.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
